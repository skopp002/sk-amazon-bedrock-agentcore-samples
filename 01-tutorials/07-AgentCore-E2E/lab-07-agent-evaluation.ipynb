{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Customer Support Agent Evaluation Framework\n",
    "\n",
    "## Overview\n",
    "\n",
    "This notebook implements the [Strands Agents evaluation strategy](https://strandsagents.com/0.1.x/documentation/docs/user-guide/observability-evaluation/evaluation/?h=evaluation) with LLM Judge Evaluation to comprehensively assess your customer support agent running on Amazon Bedrock AgentCore Runtime.\n",
    "\n",
    "### Evaluation Strategy\n",
    "\n",
    "- **Multi-dimensional Quality Assessment**: Helpfulness, accuracy, clarity, professionalism, completeness\n",
    "- **Tool Usage Analysis**: Appropriate tool selection and usage patterns\n",
    "- **Performance Metrics**: Response times and success rates\n",
    "- **LLM-as-Judge**: Claude 4 Sonnet for objective evaluation\n",
    "- **Summarize test results**\n",
    "- **Save outputs to a file**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: boto3 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (1.40.21)\n",
      "Requirement already satisfied: requests in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (2.32.4)\n",
      "Requirement already satisfied: strands-agents in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (1.6.0)\n",
      "Requirement already satisfied: bedrock-agentcore in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (0.1.2)\n",
      "Requirement already satisfied: botocore<1.41.0,>=1.40.21 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from boto3) (1.40.21)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from boto3) (1.0.1)\n",
      "Requirement already satisfied: s3transfer<0.14.0,>=0.13.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from boto3) (0.13.0)\n",
      "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from botocore<1.41.0,>=1.40.21->boto3) (2.9.0.post0)\n",
      "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from botocore<1.41.0,>=1.40.21->boto3) (2.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.41.0,>=1.40.21->boto3) (1.17.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from requests) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from requests) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from requests) (2025.7.14)\n",
      "Requirement already satisfied: docstring-parser<1.0,>=0.15 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from strands-agents) (0.16)\n",
      "Requirement already satisfied: mcp<2.0.0,>=1.11.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from strands-agents) (1.11.0)\n",
      "Requirement already satisfied: opentelemetry-api<2.0.0,>=1.30.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from strands-agents) (1.33.1)\n",
      "Requirement already satisfied: opentelemetry-instrumentation-threading<1.00b0,>=0.51b0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from strands-agents) (0.54b1)\n",
      "Requirement already satisfied: opentelemetry-sdk<2.0.0,>=1.30.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from strands-agents) (1.33.1)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.4.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from strands-agents) (2.11.7)\n",
      "Requirement already satisfied: typing-extensions<5.0.0,>=4.13.2 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from strands-agents) (4.14.1)\n",
      "Requirement already satisfied: watchdog<7.0.0,>=6.0.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from strands-agents) (6.0.0)\n",
      "Requirement already satisfied: anyio>=4.5 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (4.9.0)\n",
      "Requirement already satisfied: httpx-sse>=0.4 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (0.4.1)\n",
      "Requirement already satisfied: httpx>=0.27 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (0.28.1)\n",
      "Requirement already satisfied: jsonschema>=4.20.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (4.24.0)\n",
      "Requirement already satisfied: pydantic-settings>=2.5.2 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (2.10.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.9 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (0.0.20)\n",
      "Requirement already satisfied: sse-starlette>=1.6.1 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (2.4.1)\n",
      "Requirement already satisfied: starlette>=0.27 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (0.47.1)\n",
      "Requirement already satisfied: uvicorn>=0.23.1 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from mcp<2.0.0,>=1.11.0->strands-agents) (0.35.0)\n",
      "Requirement already satisfied: deprecated>=1.2.6 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents) (1.2.18)\n",
      "Requirement already satisfied: importlib-metadata<8.7.0,>=6.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from opentelemetry-api<2.0.0,>=1.30.0->strands-agents) (8.6.1)\n",
      "Requirement already satisfied: zipp>=3.20 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from importlib-metadata<8.7.0,>=6.0->opentelemetry-api<2.0.0,>=1.30.0->strands-agents) (3.23.0)\n",
      "Requirement already satisfied: opentelemetry-instrumentation==0.54b1 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents) (0.54b1)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.0.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents) (1.17.2)\n",
      "Requirement already satisfied: opentelemetry-semantic-conventions==0.54b1 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.54b1->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents) (0.54b1)\n",
      "Requirement already satisfied: packaging>=18.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from opentelemetry-instrumentation==0.54b1->opentelemetry-instrumentation-threading<1.00b0,>=0.51b0->strands-agents) (25.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from pydantic<3.0.0,>=2.4.0->strands-agents) (0.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from anyio>=4.5->mcp<2.0.0,>=1.11.0->strands-agents) (1.3.1)\n",
      "Requirement already satisfied: httpcore==1.* in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from httpx>=0.27->mcp<2.0.0,>=1.11.0->strands-agents) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.27->mcp<2.0.0,>=1.11.0->strands-agents) (0.16.0)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents) (25.3.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents) (2025.4.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents) (0.36.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from jsonschema>=4.20.0->mcp<2.0.0,>=1.11.0->strands-agents) (0.26.0)\n",
      "Requirement already satisfied: python-dotenv>=0.21.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from pydantic-settings>=2.5.2->mcp<2.0.0,>=1.11.0->strands-agents) (1.1.1)\n",
      "Requirement already satisfied: click>=7.0 in /Users/skoppar/workspace/sk-amazon-bedrock-agentcore-samples/.parent_venv/lib/python3.12/site-packages (from uvicorn>=0.23.1->mcp<2.0.0,>=1.11.0->strands-agents) (8.2.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install boto3 requests strands-agents bedrock-agentcore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Dependencies loaded successfully\n"
     ]
    }
   ],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import time\n",
    "import uuid\n",
    "import boto3\n",
    "import requests\n",
    "from dataclasses import dataclass, asdict\n",
    "from typing import Dict, List, Any, Optional\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "# AWS clients\n",
    "bedrock = boto3.client('bedrock-runtime')\n",
    "ssm = boto3.client('ssm')\n",
    "\n",
    "print(\"‚úÖ Dependencies loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "AGENT_NAME = \"customer_support_agent\" \n",
    "EVALUATOR_MODEL = \"us.anthropic.claude-sonnet-4-20250514-v1:0\"\n",
    "\n",
    "# Helper function to get SSM parameters\n",
    "def get_ssm_parameter(name: str) -> str:\n",
    "    try:\n",
    "        response = ssm.get_parameter(Name=name, WithDecryption=True)\n",
    "        return response['Parameter']['Value']\n",
    "    except Exception as e:\n",
    "        raise Exception(f\"Failed to get SSM parameter {name}: {e}\")\n",
    "\n",
    "# Get agent endpoint\n",
    "try:\n",
    "    AGENT_ENDPOINT = get_ssm_parameter(f\"/app/customersupport/agentcore/runtime_arn\") # This is the default value set in lab-04-agentcore-runtime.ipynb \n",
    "    # print(f\"‚úÖ Agent endpoint: {AGENT_ENDPOINT}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ö†Ô∏è  Could not get agent endpoint: {e}\")\n",
    "    AGENT_ENDPOINT = \"http://localhost:8080\"  # Fallback for local testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Classes and Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Loaded 3 test cases\n"
     ]
    }
   ],
   "source": [
    "@dataclass\n",
    "class TestCase:\n",
    "    id: str\n",
    "    query: str\n",
    "    category: str\n",
    "    expected_tools: List[str]\n",
    "    expected_criteria: Dict[str, Any]\n",
    "    description: str\n",
    "\n",
    "@dataclass\n",
    "class EvaluationResult:\n",
    "    test_case_id: str\n",
    "    query: str\n",
    "    response: str\n",
    "    metrics: Dict[str, float]\n",
    "    response_time: float\n",
    "    success: bool\n",
    "    error_message: Optional[str] = None\n",
    "    tool_calls: List[str] = None\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return asdict(self)\n",
    "\n",
    "# Test cases\n",
    "TEST_CASES = [\n",
    "    TestCase(\n",
    "        id=\"basic_greeting\",\n",
    "        query=\"Hi, I need help with my account\",\n",
    "        category=\"basic_inquiry\",\n",
    "        expected_tools=[],\n",
    "        expected_criteria={\"should_be_polite\": True, \"should_ask_for_details\": True},\n",
    "        description=\"Basic greeting and help request\"\n",
    "    ),\n",
    "    TestCase(\n",
    "        id=\"warranty_check\",\n",
    "        query=\"I have a Gaming Console Pro device, I want to check my warranty status, warranty serial number is MNO33333333\",\n",
    "        category=\"warranty_inquiry\",\n",
    "        expected_tools=[\"warranty_check\", \"gateway_tool\"],\n",
    "        expected_criteria={\"should_use_serial_number\": True, \"should_provide_warranty_status\": True},\n",
    "        description=\"Warranty status check with serial number\"\n",
    "    ),\n",
    "    # TestCase(\n",
    "    #     id=\"calendar_check\",\n",
    "    #     query=\"What's my agenda for today?\",\n",
    "    #     category=\"calendar_integration\",\n",
    "    #     expected_tools=[\"google_calendar\", \"calendar_tool\"],\n",
    "    #     expected_criteria={\"should_access_calendar\": True, \"should_show_events\": True},\n",
    "    #     description=\"Calendar agenda inquiry\"\n",
    "    # ),\n",
    "    # TestCase(\n",
    "    #     id=\"knowledge_base_query\",\n",
    "    #     query=\"What are the warranty support guidelines?\",\n",
    "    #     category=\"knowledge_base\",\n",
    "    #     expected_tools=[\"knowledge_base\", \"bedrock_kb\"],\n",
    "    #     expected_criteria={\"should_search_knowledge_base\": True, \"should_provide_guidelines\": True},\n",
    "    #     description=\"Knowledge base query for guidelines\"\n",
    "    # ),\n",
    "    TestCase(\n",
    "        id=\"troubleshooting\",\n",
    "        query=\"I have overheating issues with my device, help me debug\",\n",
    "        category=\"technical_support\",\n",
    "        expected_tools=[\"knowledge_base\", \"troubleshooting_tool\"],\n",
    "        expected_criteria={\"should_ask_device_details\": True, \"should_provide_steps\": True},\n",
    "        description=\"Technical troubleshooting assistance\"\n",
    "    )\n",
    "]\n",
    "\n",
    "print(f\"‚úÖ Loaded {len(TEST_CASES)} test cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agent Invocation and Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Agent invocation functions defined\n"
     ]
    }
   ],
   "source": [
    "# Initialize AgentCore Runtime client\n",
    "from bedrock_agentcore_starter_toolkit import Runtime\n",
    "from lab_helpers.utils import reauthenticate_user, get_existing_cognito_config\n",
    "import os\n",
    "import time\n",
    "import uuid \n",
    "from typing import Dict, List, Any, Optional\n",
    "import boto3\n",
    "# Enable verbose logging for requests\n",
    "import logging\n",
    "## Set the below config parameters if you want to see detailed logs from the agent\n",
    "#logging.basicConfig(level=logging.DEBUG)\n",
    "#logging.getLogger(\"urllib3.connectionpool\").setLevel(logging.DEBUG)\n",
    "import urllib.parse\n",
    "\n",
    "session_id = uuid.uuid4()\n",
    "agentcore_client = boto3.client(\n",
    "        'bedrock-agentcore',\n",
    "    )\n",
    "REGION_NAME=\"us-east-1\"\n",
    "cognito_config = get_existing_cognito_config()\n",
    "\n",
    "async def invoke_agent(query: str, actor_id: str = \"testuser\") -> Dict[str, Any]:\n",
    "    \"\"\"Invoke the agent using AgentCore Runtime SDK with JWT token\"\"\"\n",
    "    start_time = time.time()\n",
    "    \n",
    "        # Get bearer token\n",
    "    bearer_token = reauthenticate_user(\n",
    "        cognito_config.get(\"client_id\"), \n",
    "        cognito_config.get(\"client_secret\")\n",
    "    )\n",
    "        \n",
    "    #auth_token = os.environ.get('TOKEN')\n",
    "    #print(f\"Using Agent ARN from environment: {AGENT_ENDPOINT}\")\n",
    "\n",
    "    # URL encode the agent ARN\n",
    "    escaped_agent_arn = urllib.parse.quote(AGENT_ENDPOINT, safe='')\n",
    "\n",
    "    # Construct the URL\n",
    "    url = f\"https://bedrock-agentcore.{REGION_NAME}.amazonaws.com/runtimes/{escaped_agent_arn}/invocations?qualifier=DEFAULT\"\n",
    "\n",
    "    # Set up headers\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {bearer_token}\", #f\"Bearer {auth_token}\",\n",
    "        \"X-Amzn-Trace-Id\": \"1234\", \n",
    "        \"Content-Type\": \"application/json\",\n",
    "        \"X-Amzn-Bedrock-AgentCore-Runtime-Session-Id\": f\"eval-session-{uuid.uuid4()}\"\n",
    "    }\n",
    "    invoke_response = requests.post(\n",
    "        url,\n",
    "        headers=headers,\n",
    "        data=json.dumps({\"prompt\": query})\n",
    "    )\n",
    "\n",
    "    # Print response in a safe manner\n",
    "    print(f\"Status Code: {invoke_response.status_code}\")\n",
    "    print(f\"Response Headers: {dict(invoke_response.headers)}\")\n",
    "\n",
    "    if invoke_response.status_code == 200:\n",
    "        try:\n",
    "            response_data = invoke_response.json()\n",
    "            print(\"Response JSON:\", response_data)\n",
    "            \n",
    "            # Handle both string and dict responses\n",
    "            if isinstance(response_data, str):\n",
    "                response_text = response_data\n",
    "            elif isinstance(response_data, dict):\n",
    "                response_text = response_data.get(\"result\", str(response_data))\n",
    "            else:\n",
    "                response_text = str(response_data)\n",
    "                \n",
    "            return {\n",
    "                \"response\": response_text,\n",
    "                \"success\": True,\n",
    "                \"tool_calls\": extract_tool_calls(invoke_response),\n",
    "                \"response_time\": time.time() - start_time\n",
    "            }\n",
    "        except json.JSONDecodeError:\n",
    "            return {\n",
    "                \"response\": invoke_response.text,\n",
    "                \"success\": True,\n",
    "                \"tool_calls\": [],\n",
    "                \"response_time\": time.time() - start_time\n",
    "            }\n",
    "    else:\n",
    "        error_msg = f\"Error ({invoke_response.status_code}): {invoke_response.text[:500]}\"\n",
    "        print(error_msg)\n",
    "        if invoke_response.status_code >= 400:\n",
    "            print(\"Please ensure lab-04-agentcore-runtime.ipynb has been executed and agent is deployed\")\n",
    "        return {\n",
    "            \"response\": error_msg,\n",
    "            \"success\": False,\n",
    "            \"tool_calls\": [],\n",
    "            \"response_time\": time.time() - start_time\n",
    "        }\n",
    "    \n",
    "    \n",
    "def extract_tool_calls(response_data) -> List[str]:\n",
    "    \"\"\"Extract tool calls from agent response\"\"\"\n",
    "    tool_calls = []\n",
    "    # AgentCore response format may vary, adapt as needed\n",
    "    if hasattr(response_data, 'tool_calls'):\n",
    "        tool_calls = [str(tool) for tool in response_data.tool_calls]\n",
    "    return tool_calls\n",
    "\n",
    "print(\"‚úÖ Agent invocation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cognito_config # Ensure the output matches what you see from lab-04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "async def evaluate_response_quality(query: str, response: str, criteria: Dict[str, Any]) -> Dict[str, float]:\n",
    "    \"\"\"Evaluate response quality using Claude as judge\"\"\"\n",
    "    \n",
    "    evaluation_prompt = f\"\"\"\n",
    "    You are an expert evaluator for customer support AI agents. Evaluate the following response on a scale of 1-5 for each metric.\n",
    "\n",
    "    Customer Query: {query}\n",
    "    Agent Response: {response}\n",
    "\n",
    "    Evaluate on these metrics (1=Poor, 2=Below Average, 3=Average, 4=Good, 5=Excellent):\n",
    "\n",
    "    1. HELPFULNESS: Does the response address the customer's needs and provide useful information?\n",
    "    2. ACCURACY: Is the information provided factually correct and reliable?\n",
    "    3. CLARITY: Is the response clear, well-structured, and easy to understand?\n",
    "    4. PROFESSIONALISM: Does the response maintain appropriate tone and professionalism?\n",
    "    5. COMPLETENESS: Does the response fully address all aspects of the query?\n",
    "\n",
    "    Expected criteria: {json.dumps(criteria, indent=2)}\n",
    "\n",
    "    Respond with ONLY a JSON object in this format:\n",
    "    {{\n",
    "        \"helpfulness\": <score>,\n",
    "        \"accuracy\": <score>,\n",
    "        \"clarity\": <score>,\n",
    "        \"professionalism\": <score>,\n",
    "        \"completeness\": <score>,\n",
    "        \"reasoning\": \"Brief explanation of scores\"\n",
    "    }}\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        response_obj = bedrock.invoke_model(\n",
    "            modelId=EVALUATOR_MODEL,\n",
    "            body=json.dumps({\n",
    "                \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "                \"max_tokens\": 1000,\n",
    "                \"messages\": [\n",
    "                    {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "                ]\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        result = json.loads(response_obj['body'].read())\n",
    "        content = result['content'][0]['text']\n",
    "        \n",
    "        # Extract JSON from response\n",
    "        start_idx = content.find('{')\n",
    "        end_idx = content.rfind('}') + 1\n",
    "        json_str = content[start_idx:end_idx]\n",
    "        \n",
    "        scores = json.loads(json_str)\n",
    "        return {k: v for k, v in scores.items() if k != \"reasoning\"}\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in quality evaluation: {e}\")\n",
    "        return {\n",
    "            \"helpfulness\": 0.0,\n",
    "            \"accuracy\": 0.0,\n",
    "            \"clarity\": 0.0,\n",
    "            \"professionalism\": 0.0,\n",
    "            \"completeness\": 0.0\n",
    "        }\n",
    "\n",
    "def evaluate_tool_usage(expected_tools: List[str], actual_tools: List[str]) -> float:\n",
    "    \"\"\"Evaluate tool usage effectiveness\"\"\"\n",
    "    if not expected_tools:\n",
    "        return 5.0 if not actual_tools else 3.0\n",
    "    \n",
    "    if not actual_tools:\n",
    "        return 1.0\n",
    "    \n",
    "    expected_set = set(expected_tools)\n",
    "    actual_set = set(actual_tools)\n",
    "    \n",
    "    precision = len(expected_set.intersection(actual_set)) / len(actual_set) if actual_set else 0\n",
    "    recall = len(expected_set.intersection(actual_set)) / len(expected_set) if expected_set else 0\n",
    "    \n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    return 1 + (f1 * 4)  # Scale to 1-5\n",
    "\n",
    "print(\"‚úÖ Evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single Test Case Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Test case evaluation function defined\n"
     ]
    }
   ],
   "source": [
    "async def evaluate_test_case(test_case: TestCase) -> EvaluationResult:\n",
    "    \"\"\"Evaluate a single test case\"\"\"\n",
    "    print(f\"üîç Evaluating: {test_case.id} - {test_case.description}\")\n",
    "    \n",
    "    # Invoke agent\n",
    "    agent_result = await invoke_agent(test_case.query)\n",
    "    \n",
    "    # Handle None result\n",
    "    if agent_result is None:\n",
    "        return EvaluationResult(\n",
    "            test_case_id=test_case.id,\n",
    "            query=test_case.query,\n",
    "            response=\"\",\n",
    "            metrics={},\n",
    "            response_time=0.0,\n",
    "            success=False,\n",
    "            error_message=\"invoke_agent returned None\"\n",
    "        )\n",
    "    \n",
    "    if not agent_result[\"success\"]:\n",
    "        return EvaluationResult(\n",
    "            test_case_id=test_case.id,\n",
    "            query=test_case.query,\n",
    "            response=\"\",\n",
    "            metrics={},\n",
    "            response_time=agent_result[\"response_time\"],\n",
    "            success=False,\n",
    "            error_message=agent_result[\"error_message\"]\n",
    "        )\n",
    "    \n",
    "    # Evaluate response quality\n",
    "    quality_scores = await evaluate_response_quality(\n",
    "        test_case.query,\n",
    "        agent_result[\"response\"],\n",
    "        test_case.expected_criteria\n",
    "    )\n",
    "    \n",
    "    # Evaluate tool usage\n",
    "    tool_score = evaluate_tool_usage(\n",
    "        test_case.expected_tools,\n",
    "        agent_result[\"tool_calls\"]\n",
    "    )\n",
    "    \n",
    "    # Combine all metrics\n",
    "    metrics = {\n",
    "        **quality_scores,\n",
    "        \"tool_usage\": tool_score,\n",
    "        \"response_time\": agent_result[\"response_time\"]\n",
    "    }\n",
    "    \n",
    "    return EvaluationResult(\n",
    "        test_case_id=test_case.id,\n",
    "        query=test_case.query,\n",
    "        response=agent_result[\"response\"],\n",
    "        metrics=metrics,\n",
    "        response_time=agent_result[\"response_time\"],\n",
    "        success=True,\n",
    "        tool_calls=agent_result[\"tool_calls\"]\n",
    "    )\n",
    "\n",
    "print(\"‚úÖ Test case evaluation function defined\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Single Test Case (Demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Evaluating: basic_greeting - Basic greeting and help request\n",
      "Status Code: 200\n",
      "Response Headers: {'Date': 'Tue, 02 Sep 2025 16:25:40 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'b1dc031e-b9f1-49da-82da-db034ef045ca', 'baggage': 'session.id=eval-session-751459f6-8a6a-498d-9d1a-41c4368c80ab', 'X-Amzn-Bedrock-AgentCore-Runtime-Session-Id': 'eval-session-751459f6-8a6a-498d-9d1a-41c4368c80ab'}\n",
      "Response JSON: Hi there! I'd be happy to help you with your account. To assist you better, could you please let me know what specific aspect of your account you need help with? For example, are you looking for:\n",
      "\n",
      "- Information about your Gaming Console Pro device warranty\n",
      "- Help with account login issues\n",
      "- Order tracking information\n",
      "- Return policy questions\n",
      "- Technical support for your device\n",
      "- Something else related to your account\n",
      "\n",
      "The more details you can provide, the better I'll be able to assist you with your specific needs.\n",
      "\n",
      "üìä Demo Result for 'basic_greeting':\n",
      "Query: Hi, I need help with my account\n",
      "Response: Hi there! I'd be happy to help you with your account. To assist you better, could you please let me know what specific aspect of your account you need help with? For example, are you looking for:\n",
      "\n",
      "- I...\n",
      "Response Time: 5.940s\n",
      "Tool Calls: []\n"
     ]
    }
   ],
   "source": [
    "# Test a single case first\n",
    "demo_test = TEST_CASES[0]  # Basic greeting\n",
    "demo_result = await evaluate_test_case(demo_test)\n",
    "\n",
    "print(f\"\\nüìä Demo Result for '{demo_test.id}':\")\n",
    "print(f\"Query: {demo_result.query}\")\n",
    "print(f\"Response: {demo_result.response[:200]}...\" if len(demo_result.response) > 200 else f\"Response: {demo_result.response}\")\n",
    "print(f\"Response Time: {demo_result.response_time:.3f}s\")\n",
    "print(f\"Tool Calls: {demo_result.tool_calls}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full Evaluation Suite"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Full evaluation functions defined\n"
     ]
    }
   ],
   "source": [
    "async def run_full_evaluation(test_cases: List[TestCase]) -> Dict[str, Any]:\n",
    "    \"\"\"Run evaluation on all test cases\"\"\"\n",
    "    print(f\"üöÄ Starting evaluation of {len(test_cases)} test cases...\")\n",
    "    \n",
    "    results = []\n",
    "    for i, test_case in enumerate(test_cases, 1):\n",
    "        print(f\"\\n[{i}/{len(test_cases)}] Processing: {test_case.id}\")\n",
    "        result = await evaluate_test_case(test_case)\n",
    "        results.append(result)\n",
    "        \n",
    "        # Brief pause between tests\n",
    "        await asyncio.sleep(1)\n",
    "    \n",
    "    # Calculate summary statistics\n",
    "    summary = calculate_summary(results)\n",
    "    \n",
    "    return {\n",
    "        \"agent_name\": AGENT_NAME,\n",
    "        \"total_test_cases\": len(test_cases),\n",
    "        \"results\": [result.to_dict() for result in results],\n",
    "        \"summary\": summary,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }\n",
    "\n",
    "def calculate_summary(results: List[EvaluationResult]) -> Dict[str, Any]:\n",
    "    \"\"\"Calculate summary statistics\"\"\"\n",
    "    successful_results = [r for r in results if r.success]\n",
    "    \n",
    "    if not successful_results:\n",
    "        return {\"error\": \"No successful test cases\"}\n",
    "    \n",
    "    # Average scores\n",
    "    metrics = [\"helpfulness\", \"accuracy\", \"clarity\", \"professionalism\", \"completeness\", \"tool_usage\"]\n",
    "    avg_scores = {}\n",
    "    \n",
    "    for metric in metrics:\n",
    "        scores = [r.metrics.get(metric, 0) for r in successful_results if metric in r.metrics]\n",
    "        avg_scores[metric] = sum(scores) / len(scores) if scores else 0\n",
    "    \n",
    "    # Response time statistics\n",
    "    response_times = sorted([r.response_time for r in successful_results])\n",
    "    n = len(response_times)\n",
    "    \n",
    "    percentiles = {\n",
    "        \"p50\": response_times[n//2] if n > 0 else 0,\n",
    "        \"p90\": response_times[int(n*0.9)] if n > 0 else 0,\n",
    "        \"p95\": response_times[int(n*0.95)] if n > 0 else 0,\n",
    "        \"p99\": response_times[int(n*0.99)] if n > 0 else 0,\n",
    "    }\n",
    "    \n",
    "    return {\n",
    "        \"success_rate\": len(successful_results) / len(results),\n",
    "        \"average_scores\": avg_scores,\n",
    "        \"overall_score\": sum(avg_scores.values()) / len(avg_scores) if avg_scores else 0,\n",
    "        \"response_time_percentiles\": percentiles,\n",
    "        \"total_successful\": len(successful_results),\n",
    "        \"total_failed\": len(results) - len(successful_results)\n",
    "    }\n",
    "\n",
    "print(\"‚úÖ Full evaluation functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üöÄ Starting evaluation of 3 test cases...\n",
      "\n",
      "[1/3] Processing: basic_greeting\n",
      "üîç Evaluating: basic_greeting - Basic greeting and help request\n",
      "Status Code: 200\n",
      "Response Headers: {'Date': 'Tue, 02 Sep 2025 16:25:50 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'x-amzn-RequestId': '84e1f524-732d-4d98-b713-04874cd52d5b', 'baggage': 'session.id=eval-session-08183bc6-09c1-4331-823a-d154b980f823', 'X-Amzn-Bedrock-AgentCore-Runtime-Session-Id': 'eval-session-08183bc6-09c1-4331-823a-d154b980f823'}\n",
      "Response JSON: Hello! I'd be happy to help you with your account. To better assist you, could you please let me know what specific account issue you're having? For example:\n",
      "\n",
      "- Are you having trouble logging in?\n",
      "- Do you need to update your account information?\n",
      "- Are you looking for order history or tracking?\n",
      "- Do you want to check warranty information for your Gaming Console Pro?\n",
      "- Are you having issues with payments or billing?\n",
      "\n",
      "Once I know more about your specific needs, I can provide you with the most appropriate assistance.\n",
      "\n",
      "[2/3] Processing: warranty_check\n",
      "üîç Evaluating: warranty_check - Warranty status check with serial number\n",
      "Status Code: 200\n",
      "Response Headers: {'Date': 'Tue, 02 Sep 2025 16:26:05 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'x-amzn-RequestId': '11f0cd20-3234-43de-8787-285f4729c6e8', 'baggage': 'session.id=eval-session-2d8b1711-ad8a-4f14-8f4c-bea5196a6792', 'X-Amzn-Bedrock-AgentCore-Runtime-Session-Id': 'eval-session-2d8b1711-ad8a-4f14-8f4c-bea5196a6792'}\n",
      "Response JSON: I apologize, but I'm unable to access the specific warranty information for your Gaming Console Pro with serial number MNO33333333 directly through our system. For checking the warranty status of your specific device, you'll need to contact our technical support team who can look up your serial number in our warranty database.\n",
      "\n",
      "Here's how you can get your warranty status verified:\n",
      "\n",
      "1. **Contact our Technical Support Team:**\n",
      "   - Call: 1-800-TECH-HELP (1-800-832-4435)\n",
      "   - Email: support@electronicsstore.com\n",
      "   - Live Chat: Available on our website 24/7\n",
      "\n",
      "2. **Information to have ready:**\n",
      "   - Your Gaming Console Pro serial number: MNO33333333\n",
      "   - Proof of purchase (if available)\n",
      "   - Your contact information\n",
      "\n",
      "Our technical support team will be able to verify your warranty status immediately and provide you with information about any remaining coverage period, what's covered under your warranty, and options if your warranty has expired.\n",
      "\n",
      "Is there anything else I can help you with regarding your Gaming Console Pro or any other electronics products?\n",
      "\n",
      "[3/3] Processing: troubleshooting\n",
      "üîç Evaluating: troubleshooting - Technical troubleshooting assistance\n",
      "Status Code: 200\n",
      "Response Headers: {'Date': 'Tue, 02 Sep 2025 16:26:27 GMT', 'Content-Type': 'application/json', 'Transfer-Encoding': 'chunked', 'Connection': 'keep-alive', 'x-amzn-RequestId': 'e3423417-025c-4f58-8780-d71467dfdc50', 'baggage': 'session.id=eval-session-b28c4f95-7108-48ca-b37e-022269414f7d', 'X-Amzn-Bedrock-AgentCore-Runtime-Session-Id': 'eval-session-b28c4f95-7108-48ca-b37e-022269414f7d'}\n",
      "Response JSON: ## MacBook Pro Overheating Troubleshooting Guide\n",
      "\n",
      "Based on your issue with MacBook Pro overheating during video editing, here's a systematic approach to diagnose and resolve the problem:\n",
      "\n",
      "### Immediate Steps:\n",
      "1. **Check Running Applications**: \n",
      "   - Open Activity Monitor to identify CPU/GPU-intensive processes\n",
      "   - Look for background processes consuming excessive resources\n",
      "   - Force quit unnecessary applications\n",
      "\n",
      "2. **Verify Ventilation**:\n",
      "   - Ensure your MacBook is on a hard, flat surface\n",
      "   - Check that vents aren't blocked or covered\n",
      "   - Elevate the laptop slightly to improve airflow\n",
      "\n",
      "3. **Clean Cooling System**:\n",
      "   - Power off completely\n",
      "   - Gently clean external vents using compressed air\n",
      "   - If comfortable, remove bottom panel to clean internal fans (or seek professional help)\n",
      "\n",
      "4. **Software Solutions**:\n",
      "   - Install any pending macOS updates\n",
      "   - Reset SMC (System Management Controller) which controls thermal management\n",
      "   - Monitor temperatures using apps like \"Macs Fan Control\" or \"iStat Menus\"\n",
      "\n",
      "5. **Video Editing Specific**:\n",
      "   - Check if your editing software has efficiency/performance modes\n",
      "   - Reduce preview resolution while editing\n",
      "   - Consider proxy workflow for high-resolution footage\n",
      "   - Ensure you're using proper hardware acceleration settings\n",
      "\n",
      "### Advanced Solutions:\n",
      "- Apply new thermal paste if you're technically inclined (or seek professional help)\n",
      "- Consider an external cooling pad designed for laptops\n",
      "- Optimize your video editing workflow for less GPU/CPU intensive operations\n",
      "- Check for malware that could be causing background processes to run\n",
      "\n",
      "Would you like more specific guidance on any of these troubleshooting steps? Also, could you share which model of MacBook Pro you have and what video editing software you're using? This would help me provide more targeted advice.\n",
      "\n",
      "============================================================\n",
      "üìä EVALUATION COMPLETE\n",
      "============================================================\n",
      "\n",
      "ü§ñ LLM JUDGE SCORES BY TEST CASE:\n",
      "------------------------------------------------------------\n",
      "\n",
      "üìù BASIC_GREETING:\n",
      "   Helpfulness:     4.0/5.0\n",
      "   Accuracy:        4.0/5.0\n",
      "   Clarity:         5.0/5.0\n",
      "   Professionalism: 5.0/5.0\n",
      "   Completeness:    3.0/5.0\n",
      "   Tool Usage:      5.0/5.0\n",
      "   üìä Average:      4.3/5.0\n",
      "\n",
      "üìù WARRANTY_CHECK:\n",
      "   Helpfulness:     3.0/5.0\n",
      "   Accuracy:        3.0/5.0\n",
      "   Clarity:         4.0/5.0\n",
      "   Professionalism: 4.0/5.0\n",
      "   Completeness:    2.0/5.0\n",
      "   Tool Usage:      1.0/5.0\n",
      "   üìä Average:      2.8/5.0\n",
      "\n",
      "üìù TROUBLESHOOTING:\n",
      "   Helpfulness:     4.0/5.0\n",
      "   Accuracy:        4.0/5.0\n",
      "   Clarity:         5.0/5.0\n",
      "   Professionalism: 4.0/5.0\n",
      "   Completeness:    4.0/5.0\n",
      "   Tool Usage:      1.0/5.0\n",
      "   üìä Average:      3.7/5.0\n"
     ]
    }
   ],
   "source": [
    "# Run full evaluation\n",
    "evaluation_results = await run_full_evaluation(TEST_CASES)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"üìä EVALUATION COMPLETE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Display LLM Judge scores for each test\n",
    "print(\"\\nü§ñ LLM JUDGE SCORES BY TEST CASE:\")\n",
    "print(\"-\" * 60)\n",
    "for result in evaluation_results.get(\"results\", []):\n",
    "    if result[\"success\"] and result.get(\"metrics\"):\n",
    "        metrics = result[\"metrics\"]\n",
    "        print(f\"\\nüìù {result['test_case_id'].upper()}:\")\n",
    "        print(f\"   Helpfulness:     {metrics.get('helpfulness', 0):.1f}/5.0\")\n",
    "        print(f\"   Accuracy:        {metrics.get('accuracy', 0):.1f}/5.0\")\n",
    "        print(f\"   Clarity:         {metrics.get('clarity', 0):.1f}/5.0\")\n",
    "        print(f\"   Professionalism: {metrics.get('professionalism', 0):.1f}/5.0\")\n",
    "        print(f\"   Completeness:    {metrics.get('completeness', 0):.1f}/5.0\")\n",
    "        print(f\"   Tool Usage:      {metrics.get('tool_usage', 0):.1f}/5.0\")\n",
    "        avg_score = sum([v for k, v in metrics.items() if k != 'response_time']) / 6\n",
    "        print(f\"   üìä Average:      {avg_score:.1f}/5.0\")\n",
    "    else:\n",
    "        print(f\"\\n‚ùå {result['test_case_id'].upper()}: FAILED\")\n",
    "        print(f\"   Error: {result.get('error_message', 'Unknown error')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ Agent: customer_support_agent\n",
      "üìù Total Test Cases: 3\n",
      "‚úÖ Success Rate: 100.0%\n",
      "üéØ Overall Score: 3.61/5.0\n",
      "\n",
      "üìà QUALITY METRICS (1-5 scale):\n",
      "  üü° Helpfulness: 3.67\n",
      "  üü° Accuracy: 3.67\n",
      "  üü¢ Clarity: 4.67\n",
      "  üü¢ Professionalism: 4.33\n",
      "  üü° Completeness: 3.00\n",
      "  üî¥ Tool_Usage: 2.33\n",
      "\n",
      "‚è±Ô∏è  RESPONSE TIME PERCENTILES:\n",
      "  P50: 11.404s\n",
      "  P90: 17.285s\n",
      "  P95: 17.285s\n",
      "  P99: 17.285s\n",
      "\n",
      "üìã DETAILED RESULTS:\n",
      "  ‚úÖ basic_greeting: 4.59/5.0 (6.150s)\n",
      "  ‚úÖ warranty_check: 4.06/5.0 (11.404s)\n",
      "  ‚úÖ troubleshooting: 5.61/5.0 (17.285s)\n"
     ]
    }
   ],
   "source": [
    "def print_detailed_summary(results: dict):\n",
    "    \"\"\"Print comprehensive evaluation summary\"\"\"\n",
    "    summary = results.get(\"summary\", {})\n",
    "    \n",
    "    print(f\"ü§ñ Agent: {results['agent_name']}\")\n",
    "    print(f\"üìù Total Test Cases: {results['total_test_cases']}\")\n",
    "    print(f\"‚úÖ Success Rate: {summary.get('success_rate', 0):.1%}\")\n",
    "    print(f\"üéØ Overall Score: {summary.get('overall_score', 0):.2f}/5.0\")\n",
    "    \n",
    "    print(\"\\nüìà QUALITY METRICS (1-5 scale):\")\n",
    "    avg_scores = summary.get(\"average_scores\", {})\n",
    "    for metric, score in avg_scores.items():\n",
    "        if metric != \"response_time\":\n",
    "            emoji = \"üü¢\" if score >= 4.0 else \"üü°\" if score >= 3.0 else \"üî¥\"\n",
    "            print(f\"  {emoji} {metric.title()}: {score:.2f}\")\n",
    "    \n",
    "    print(\"\\n‚è±Ô∏è  RESPONSE TIME PERCENTILES:\")\n",
    "    percentiles = summary.get(\"response_time_percentiles\", {})\n",
    "    for p, time_val in percentiles.items():\n",
    "        print(f\"  {p.upper()}: {time_val:.3f}s\")\n",
    "    \n",
    "    print(\"\\nüìã DETAILED RESULTS:\")\n",
    "    for result in results.get(\"results\", []):\n",
    "        status = \"‚úÖ\" if result[\"success\"] else \"‚ùå\"\n",
    "        if result[\"success\"] and result.get(\"metrics\"):\n",
    "            score = sum(result[\"metrics\"].values()) / len(result[\"metrics\"])\n",
    "            print(f\"  {status} {result['test_case_id']}: {score:.2f}/5.0 ({result['response_time']:.3f}s)\")\n",
    "        else:\n",
    "            print(f\"  {status} {result['test_case_id']}: FAILED - {result.get('error_message', 'Unknown error')}\")\n",
    "\n",
    "# Print detailed summary\n",
    "print_detailed_summary(evaluation_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üíæ Results saved to: evaluation_results_customer_support_agent_20250902_092632.json\n"
     ]
    }
   ],
   "source": [
    "# Save results to file\n",
    "output_file = f\"evaluation_results_{AGENT_NAME}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json\"\n",
    "\n",
    "with open(output_file, 'w') as f:\n",
    "    json.dump(evaluation_results, f, indent=2)\n",
    "\n",
    "print(f\"üíæ Results saved to: {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".parent_venv (3.12.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
